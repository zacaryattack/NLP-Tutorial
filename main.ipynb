{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         id  target\n",
      "0         0       0\n",
      "1         2       1\n",
      "2         3       1\n",
      "3         9       0\n",
      "4        11       1\n",
      "...     ...     ...\n",
      "3258  10861       1\n",
      "3259  10865       1\n",
      "3260  10868       1\n",
      "3261  10874       1\n",
      "3262  10875       0\n",
      "\n",
      "[3263 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sys import path\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "\n",
    "\n",
    "\n",
    "def p(item_list=None, snlc=0, title=None):\n",
    "\n",
    "    if snlc > 0:\n",
    "        print('\\n'*snlc, end='')\n",
    "\n",
    "    if title != None:\n",
    "        print(title, end='')\n",
    "\n",
    "    item_list = [item_list] if type(item_list) is not list else item_list\n",
    "    for item in item_list:\n",
    "        print('\\n', item, '\\n')\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "NLP_Tutorial_path = path[0]\n",
    "\n",
    "train_data_path = NLP_Tutorial_path + '/train.csv'\n",
    "train_df = pd.read_csv(train_data_path)\n",
    "\n",
    "test_data_path = NLP_Tutorial_path + '/test.csv'\n",
    "test_df = pd.read_csv(test_data_path)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "FAKE_disaster_tweet = train_df[train_df['target'] == 0]['text'].values[1]\n",
    "#p(FAKE_disaster_tweet, title='FAKE_disaster_tweet')\n",
    "\n",
    "REAL_disaster_tweet = train_df[train_df['target'] == 1]['text'].values[1]\n",
    "#p(REAL_disaster_tweet, title='REAL_disaster_tweet')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Using scikit-learn's CountVectorizer to count the words in each tweet\n",
    "# and turn them into data the machine learning model can process.\n",
    "count_vectorizer = feature_extraction.text.CountVectorizer()\n",
    "#p(count_vectorizer, title='count_vectorizer')\n",
    "\n",
    "\n",
    "\n",
    "# let's get counts for the first 5 tweets in the data\n",
    "example_train_vectors = count_vectorizer.fit_transform(train_df['text'][0:5])\n",
    "#p(example_train_vectors, snlc=3, title='example_train_vectors')\n",
    "\n",
    "\n",
    "\n",
    "# we use .todense() here because these vectors are \"sparse\" (only non-zero elements are kept to save space)\n",
    "result = example_train_vectors[0].todense().shape\n",
    "#p(result, snlc=3, title='example_train_vectors[0].todense().shape')\n",
    "result = example_train_vectors[0].todense()\n",
    "#p(result, title='example_train_vectors[0].todense()')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# vectors for all of our tweets\n",
    "train_vectors = count_vectorizer.fit_transform(train_df['text'])\n",
    "#p(train_vectors, snlc=5)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Note that we're NOT using .fit_transform() here.\n",
    "# Using just .transform() makes sure that the tokens in the train vectors are the only ones \n",
    "# mapped to the test vectors - i.e. that the train and test vectors use the same set of tokens.\n",
    "test_vectors = count_vectorizer.transform(test_df['text'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Becasue our vectors are really big, we want to push our model's weights\n",
    "# toward 0 without completely discounting different words\n",
    "# - ridge regression is a good way to do this.\n",
    "Classifier = linear_model.RidgeClassifier()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#scores\n",
    "scores = model_selection.cross_val_score(Classifier, train_vectors, train_df['target'], cv=3, scoring='f1')\n",
    "#p(scores)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Classifier.fit(train_vectors, train_df['target'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sample_submission_path = NLP_Tutorial_path + '/sample_submission.csv'\n",
    "sample_submission = pd.read_csv(sample_submission_path)\n",
    "#p(sample_submission)\n",
    "\n",
    "sample_submission['target'] = Classifier.predict(test_vectors)\n",
    "#p(sample_submission['target'])\n",
    "\n",
    "sample_submission.head()\n",
    "\n",
    "submission_path = NLP_Tutorial_path + '/submission.csv'\n",
    "sample_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "\n",
    "print(sample_submission)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
